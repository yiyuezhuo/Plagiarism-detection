语音识别四种分类方法的综述

1.研究的主要内容和意义

移动互联网已迅速成为当今发展最快、规模最大和市场前景最好的行业之一，己吸引了众多知名公司和科研机构进军该领域。由于现有的移动终端设备交互方式存在很多的局限，如键盘输入文字不便，操作繁琐等，而语音是人机交互最自然的方式，近些年来，正在日益影响和改变人们的日常生活。移动互联网对语音技术本身有着天然的需求，随着移动互联时代的到来，移动终端作为人手必备的工具，语音技术带来的交互优势更加明显，可以很大程度地提高移动终端交互效率和改善交互体验。语音技术包括语音识别和语音合成技术，这两个技术使得移动终端具备了能听会说的能力。

让计算机能听懂人类的语言，是人类自计算机诞生以来梦寐以求的愿望。随着计算机越来越向便携化方向发展，随着计算环境的日趋复杂，人们越来越迫切地要求摆脱键盘的束缚而代之以语音输入这样简便使用的、自然的、人性化的输入方式[1]。作为高科技应用领域的研究热点，语音识别(Speech Recognition)技术从理论的研究到产品的开发已经走过了四十多个春秋并且取得了长足的进步，它正在直接与办公或商业系统的数据库语音查询、数据库维护与管理、语音输入、工业生产部门的语声控制(Command＆Contr01)、电话与电信系统的自动拨号、辅助控制与查询以及医疗与卫生部门的专业报告的语音创建与编辑等各种实际应用相接轨，并且极有可能成为下一代操作系统和应用程序的用户界面。据统计，全世界的语音识别市场正在以每年20％的速度飞速增长。与此同时，目前的语音识别技术研究水平还远不能达到使计算机与人类之间能够自然交流的这个终极目标，甚至曾有专家比喻其难度要超过“人类登上月球”!可见，语音识别技术的研究将是一项极具市场价值和挑战性的工作。我们今天进行这一领域的研究与开拓就是要让语音识别技术走入人们的日常生活当中，并不断朝更高目标而努力。

2.国内外研究现状和发展水平

本节介绍语音识别在国内外的发展，主要有科大讯飞、苹果公司手机Siri个人助手、谷歌语音搜索与Voice Actions、微软语音搜索软件Tellme。

科大讯飞的MSP(I FLY Mobile Speech Platform)[2]。MSP是一个应用于移动互联网的语音服务平台，该平台提供了架构于互联网的语音云服务和一套移动互联网语音解决方案、应用示例，把语音服务的应用范围拓宽到移动互联网领域，为语音服务产品走向移动互联网市场开辟的应用模式。MSP平台整合了科大讯飞研究院、中国科技大学讯飞语音实验室以及清华大学讯飞语音实验室在语音识别、语音合成等技术上多年的技术成果，采用分布式架构，继承了科大讯飞电信级语音平台高稳定的特点。该系统的特点是综合了很多语音处理的相关技术，包括语音识别，说话人识别，语音合成等，为语音技术的应用和推广提供很好的实例和平台，但所有核心算法都是在服务器上运行，用户必须通过网络连接到该平台的服务器上，服务器进行计算后将结果传送给用户。该模式识别速度会受到网络状况的限制，这给用户带来很多的不便。

苹果公司手机Siri个人助手[3]。Siri技术来源于美国国防部高级研究规划局所公布的CALO计划：一个让军方简化处理一些繁复事物，并具有学习、组织以及认知能力的数字助理。Siri则是其所衍生出来的民用版软件。Siri通过语音合成技术来读短信、介绍餐厅、询问天气、语音设置闹钟等；通过语音识别技术来调用系统自带的天气预报、日程安排、搜索资料等基本应用，还能够不断学习新的声音和语调，提供智能的对话式应答。该应用软件特点是友好的用户界面和体验，交互式问答模式。

谷歌语音搜索与Voice Actions[4]。该语音搜索引擎支持英文、中文以及中英混合的语音输入，并采用了噪音分离技术，从一定程度上降低了背景噪音的影响。Voice Actions是谷歌于2010年8月推出的一款应用，利用该应用可以通过语音命令控制内置应用软件，搜索地理位置。虽然Voice Actions提供了基本的语音识别引擎，不过Voice Actions要求语音输入符合严格的语法结构，否则系统将无法识别，系统鲁棒性较差。

微软语音搜索软件TelIme[5]。TelIme语音识别引擎采用云计算模式，并能通过说话人语音自适应技术，使得识别率在使用过程中得到不断提升。2010年微软将Tellme整合进Windows Phone 7，提供两种基本操作：拨打电话给联系人和启动应用程序。微软通过苹果Appstore商店发布了iPad版必应，在必应iPad版本中微软加入了语音搜索功能。

语音识别四种分类方法的比较研究

语音识别系统模型通常由声学模型和语言模型两部分组成。声学模型能否真实地反映话音的物理变化规律, 语言模型能否表达自然语言所包含的丰富语言学知识,是语音识别系统性能好坏的关键。然而语音信号和自然语言都是随机多变和不稳定的,这是目前语音识别中最大的难点。

声学模型是识别系统的底层模型,其目的是提供一种计算语音的特征矢量序列和每个发音模板之间距离的方法。人的发音在每一时刻都受到其前后发音的影响，为了模仿自然连续语音中协同发音作用和鉴别这些不同发音,通常要求使用复杂的声学模型。声学模型的设计和语言发音特点密切相关。声学模型单元大小(字发音模型、半音节模型或音素模型)对语音训练数据量大小、系统识别率、以及灵活性有较大的影响。对大词汇量语音识别系统来说，通常识别单元小，则计算量也小，所需的模型存储量也小，要求的训练数据量也少。但带来的问题是对应语音段的定位和分割较困难,识别模型规则也变得更复杂。通常大的识别单元在模型中包括协同发音，这有利于提高系统的识别率，但要求的训练数据相对增加。因此，识别单元的大小应根据语言的特点、识别系统词汇量的大小而定。

主要的语音识别分类有4种：样本匹配法、判决系统、HMM声学建模和神经网络[6]。

3.1 样本匹配法

一个典型的样本匹配过程[7]：未知(待识别)语音经过话筒变换成电信号，后加在识别系统的输入端，首先要经过预处理，预处理包括去噪、预加重、分帧、端点检测、加窗。经过预处理后的语音信号再经过特征提取(特征提取包括自相关分析、LPC分析及倒谱分析)，语音信号的特征被提取出来。常用的特征包括：短时平均能量或幅度、短时平均过零率、短时自相关函数、线性预测系数、倒谱、共振峰等。根据实际需要选择语音特征参数(本课题选择基于LPC的倒谱特征)，这些特征参数的时间序列便构成了待识别语音的模式，将其与已经存储在计算机内的参考模式逐一进行比较(模式匹配1，获得最佳匹配(由判决规则确定)的参考模式便是识别结果。参考模式是在系统使用前获得势存储起来的，为此，要输入一系列已知语音信号，提取它们的特征作为参考模式，这一过程称为训练过程。这个过程可以用图1 进行表示。



图1 基于模式匹配原理的语音识别系统的原理框图

训练过程可概括为预处理、声音截取和分帧、帧处理得到数据文件和模板的匹配判断[8]。

3.1.1 预处理

首先是对声音信号的预处理。预处理第1步是采样，按照8 kHz的采样频率进行直接采集。根据奈奎斯特定理，这个采样频率可以保证声音信号的无失真复原。第2步是对声音信号幅值的一个削弱，因为声音信号由麦克输入，声音幅值会很大，造成不必要的干扰。第3步是加窗，加汉明窗，滤掉高频成分，可以有效防止频谱的混叠。由于用麦克输入的声音信号的信噪比都比较高，所以就不用考虑滤波这个环节了。预处理这个环节就是为了给后面的各个模块提供品质较好的语音信号数据。

3.1.2 声音截取和分帧

接下来就是端点的检测。语音信号经过预处理后，要提取出字词，就必须通过进行端点检测来去除前后两端无声区的影响，使得声音信号尽可能不受人为输入反应时间的干扰。第一步是计算所有声音信号幅值的平均值a，当声音的幅值达到平均值的，我们就认为这个字开始了，当声音信号再次下降到平均值的，我们就认为这个字结束了。在这里，幅值代替了功率的作用， 和  作为判断的两个阈值。因为说话的声音的幅值肯定大于背景噪声，所以这个方法在安静环境下或者采用麦克风输入时效果还可以接受。如果环境质量不高，还可以结合过零率等参数来进行端点检测。 

3.1.3 帧处理得到数据文件

帧处理的过程如下：

第1步，对语音信号每一帧的加窗语音做256点的DFT变换，得到频域上的表达式，即功率谱。

第2步，划分临界带。

第3步，求临界带特征矢量。目前特征是语音信号预处理中的重要步骤。在实际特征提取中，较常采用的参数是线性预测倒谱系数（LPCC）和Mel倒谱系数（MFCC）[9]。二者采用的均是时域转换到倒谱域上，但是出发思路两者不同。线性预测倒谱系数（LPCC）以人类发声模型为基础，采用线性预测编码（LPC）技术求倒谱系数；Mel倒谱系数（MFCC）以人类听觉模型为基础，通过离散傅利叶变换（DFT）进行变换分析。

3.1.4 模板的匹配判断

比较语音参量我们用的是动态时间规整（Dynamic Time Warping，DTW）算法，DTW算法能够较好地解决用于孤立词识别时说话速度不均匀的难题。测试的语音参数共有N帧矢量，而参考模板有M帧矢量，且M不等于N，则DTW就是寻找一个时间归整函数，它将测试矢量的时间轴n非线性地映射到模板的时间轴m上，并使该函数满足第n帧测试矢量和第m帧模板矢量之间的距离测度最小。

在文献[10]中，提出一个多模板的优化思想。取两个模板为一个小组，采用动态规整的方法得到两模板的匹配路径。然后两模板根据匹配路径，让对应帧的特征参数相加之后取平均得到一个新模板。然后所有小组产生的新模板相加取平均，得到优化模板。这个优化模板与三个原始模板的都有很强的相关性，这种相关性与个人的发音本质相对应，原始模板与优化是一般性与特殊性的关系。这样一来，优化模板就很好地结合了多模板参与匹配的人性化思想，同时优化模板只有一个，又有了单模板匹配的简洁快速的特性。

显然，最佳匹配结果的获得与特征的选择、语音模型的好坏、模板是否准确都有直接的关系，这也是目前语音识别过程中的一个难点。

3.2  判决系统

音频判决系统[11]从音频信号和内容特征的两个方面入手，解决音频质量检测与内容检测两大问题。基于信号特征的音频判决是指系统根据不同用户的需求来设置相应的音频质量检测参数选项，通过计算机来对待检测的音频文件进行质量检测。这种判决方式不需要工作人员过多的参与，而是通过计算机来操作，但是前提是需要给出合理的算法才能达到不错的检测水平。实验室播出系统有一个对音频质量进行检测的模块，没有对音频的内容进行审核，本论文的工作主要是在完成基于信号的音频判决模块的前提下，同时提出自己的一个创新点，设计并实现出基于内容特征的音频判决模块，从而完善整个音频判决系统。基于内容特征的音频判决是指通过语音识别及关键字识别相关算法实现对一段音频的内容进行甄别，判断音频的内容是否有敏感字眼，诸如“法轮功”等，这对广播节目的安全播出尤为关键。



图2 音频判决系统的设计

如上图2，音频判决系统的音频审核部分是一系列识别检测过程，主要针对音频采样数据进行分析。一方面，如果用户选择音频质量检测，可以检测诸如静音、立体声相位反相等异常问题。另一方面，如果用户选择音频内容检测，系统先进行语音识别，将语音转化为文本，然后基于文本结果进行关键词搜索，最后对检索到的关键词进行定位。音频质量检测和音频内容检测之间是并联关系，相互独立，用户根据自己的需求选择检测项。另外音频质量检测部分的内部实现子检测项之间是相互独立的，没有一定的先后顺序。相反，音频内容检测部分的内部实现子检测项之间是串联关系，必须遵循一定的先后顺序。该系统能够支持多种格式的音频文件，对文件的检测能够精确到每帧，用户根据自己的需要来选择具体的检测项，系统的最终检测结果在软件界面上显示出来。

3.3  HMM声学建模

3.3.1 模型简介

HMM 模型是语音信号时变特征的有参表示法。它由相互关联的两个随机过程共同描述信号的统计特性，其中一个是隐蔽的(不可观测的)具有有限状态的马尔可夫链，另一个是与马尔可夫链的每一状态相关联的观察矢量的随机过程(可观测的)。隐马尔可夫链的特征要靠可观测到的信号特征揭示。这样，语音等时变信号某一段的特征就由对应状态观察符号的随机过程描述，而信号随时间的变化由隐马尔可夫链的转移概率描述。HMM模型的设计如下图3所示：



图3  HMM模型设计原型

虽然隐马尔可夫模型（HMM）是现在最流行的语音识别模型，然而基本型的HMM有一个固有的缺陷，就是它采用状态输出独立假设，影响了HMM描述语音信号时间上帧间相关动态特性的能力。为了弥补这一缺陷，已经有许多改进方法被提出。文献[12]中提到诸如：增加删状态数和在时间轴方向利用回归系数法；使用线性或非线性预测器法；利用多项式回归函数法；利用条件概率HMM的方法；利用模拟人的听觉顺向时频特性的动态倒谱系数法等。这些提案对于改善传统输出独立HMM的缺陷都是有效的方法，但是它们实现起来较为复杂。

3.3.2  HMM三个基本问题及其解决方案

1) 已知观察序列O和模型, 如何计算由此产生此观察序列概率 ?

这个问题实际上是一个模型评估问题, 因为反映了观察序列与模型吻合的程度。在语音识别中, 我们可以通过计算、比较，从多个模型参数中选择出与观察序列匹配的最好的模型。为了解决这个问题, 前人已经研究了向前向后算法。

已知观察序列O和模型 , 如何确认一个合理的状态序列, 使之能最佳地产生O，即如何选择最佳的状态序列？

这个问题关键是怎样最佳的准则来决定状态的转移。一种可能的最佳准则是:



这里存在一个问题: 有时候会出现不允许的转移, 即，那么对这些i和j所得到的状态序列就是不可能状态序列也就是说,式子得到的解只是在每个时刻决定一个最可能的状态,而没考虑整体结构, 相邻的状态和观察序列长度问题。针对这个问题,最好的解决方案是Viterbi[13]算法, 也是在语音识别过程中的主要算法。

3) 语音模型训练的好坏直接关系到语音识别系统识别率的高低。为了得到一个好的模板，往往需要有大量的原始语音数据来训练语音模型。因此，在开始进行语音识别研究之前， 首先要建立起一个庞大的语音数据库和语料数据库。一个好的语音数据库包括足够数量、具有不同性别、年龄、口音说话人的声音，并且必须要有代表性，能均衡地反映实际使用情况。有了语音数据库及语音特征，就可以建立语音模型，并用语音数据库中的语音来训练这个语音模型。训练过程是指选择系统的某种最佳状态不断地调整参数使得最大。这是一个复杂的过程，因为没有解析法可以用来求最大似然模型，所以只能用迭代法(Baum- Welch) 算法或者使用最佳梯度法。要求计算机有强大的计算能力，并有很强的理论指导，才能保证得到良好的训练结果。

3.3.3  HMM的优化

HMM是到目前为止已有的最强有力的语音识别算法。对语音识别系统而言,HMM的输出值通常就是各个帧的声学特征。为了降低模型的复杂度 , 通常 HMM模型有两个假设前提,一是内部状态的转移只与上一状态有关,一是输出值只与当前状态或当前状态转移有关。除了这两个假设外，HMM模型还存在着一些理论上的假设,其中之一就是,它假设语音是一个严格的马尔科夫过程。我们通常用从左向右的单向的、带自环的、带跨越的HMM拓扑结构来对识别基元建模。例如,一个音素对应一个三至五状态的HMM,一个词对应于构成该词的多个音素的HMM串,而连续语音则对应于词和静音组合起来的HMM串。

非齐次HMM[14]，将对HMM模型方法参数的方法进行了重新优化，假设模型的状态驻留长度分布函数，从而导出转移矩阵，已知，则：



3.4 神经网络

目前，有四类主要的神经网语音识别方法[15]。第一类方法是使用具有特殊结构的神经网络，如：回归网络(Recurrent Net)，时间延迟网络(TDNN)，动态网络(Dynamic)，这样以便处理时变语音特征序列。第二类方法是典型的Kohonen语音识别方法。这种方法基于对特征映射中特征矢量序列的轨迹的处理。它需要复杂的人工智能规则解释各序列轨迹，且依赖所需识别语言。第三类方法是神经网络与HMM(Hidden Markov Model)相结合，以神经网络有效的静态模式分类与HMM有效的时变序列建模能力相结合。第四类是以传统的DTW算法为基础，然后用神经网络的并行运算来作为硬件实现的手段。

在文献[16]中，作者谈到人工神经网络能够逼近所有的多元连续函数。通过对数据样本的学习，神经网络能自动地逼近最佳刻画样本数据规律的函数，而不必事先设想函数应具备的具体形式，能自动地建立预测模型并作出正确的预测。目前有许多种人工神经网络的模型，如前馈多层人工神经网络的模型，Hopfield人工神经网络、模糊神经网络的模型、玻尔兹曼CBM)人工神经网络的模型。前馈多层神经网络方法[17]是最早应用于语言识别的研究，也是最成功的。



图4 语音识别的前馈多层神经网络模型

采用前馈多层神经网络的模型[18]，其网络的拓朴结构见上图4。该模型中具有一个输出层，一个输入层，中间还有一个隐含层。输入层各神经元将输入信号经权重耦合到隐含层的每个节点，隐含层各节点对来自前一层的信号加权和，经Sigmoid函数转换后再耦合到输出层。输出节点将输入信号经Sigmoid函数后给出网络的输出信号,然后将网络输出信号与期望输出信号(语音者)进行比较，计算两者之间的误差(RMSE, root-mean-square error)值：



这里S和P分别表示神经网络的训练样本数和输出层节点的数目，和分别表示第个输出层节点上的个样本的期望输出值(语音者)和网络计算输出值.若RMSE大于预定值,网络进行反向传播,依次求出误差信号和误差梯度,并根据误差梯度修改权重



再进行新的权重重新进行正向处理,直到RMSE满足要求为止.这里η和μ分别表示学习步长和控制常数.输出层误差信号为



同时隐含层误差信号为



和是权重系数。通过对S个样本的训练，得到一组权重系数和。用这一组权重系数对我们要预测的参数进行预测，得到预测结果。

语音识别的一些框架

4.1 软插件

软插件[19]作为软件的一种集成机制，具有以下特征:1、模块性好，独立性强;2、可靠性好;3、内部功能的高效实现;4、连接简单，使用方便;5、有封装功能;6、清晰、简明的说明。

    使用软插件模式的例子有很多，如著名的Java开发环境Eclipse就是一个最典型的使用软插件模式的软件，各种插件集中在Eclipse的一个名称为plugins的文件夹中，以关.jar形式打包，通常还都配有相应配置文件plugin. xml用于提供插件的配置参数信息。Eclipse在启动时自动扫描此文件夹，并装入各种组件。又如图像处理软件Photoshop用于实现图像特效的滤镜，也是软插件技术装载的。

    通过分析，决定使用软插件模式来开发辅助操作部分，使系统可以承受辅助操作的功能变化。在实现的过程中模仿了Eclipse的插件体系，使用的核心技术是反射。

程序代码在编译后生成可执行的应用程序，.Net的应用程序结构分为程序域一程序集一模块一类型一成员几个层次。程序集包含模块，而模块包含类型，类型又包含成员。反射提供了封装程序集、模块和类型的对象。可以使用反射动态地创建类型的实例，将类型绑定到现有对象，或从现有对象中获取类型。然后，可以调用类型的方法或访问其字段和属性。

4.2  POCKETSPHINX框架

POCKETSPHINX[12]是CMU(Carnegie Mellon University)面向嵌入式设备的开发的大词汇量连续语音识别(LVCSR)的搜索引擎，该引擎用按ANSI标准用C语言实现，支持GNU／Linux，*BSD，Mac OS X，Android，uCl inux，Windows及Windows CE等多种平台。

POCKETSPHINX主要针对高斯混合模型(HMM)计算方面做了优化。POCKETSPHINX把GMM分割成4层计算：语音帧层、单个高斯层、GMM层、混合成分层(特征向量)，分别针对这4层计算进行了相关的优化。

























参考文献

赵力.语音信号处理.第一版,北京:机械工业出版社,2003

http://dev.voicecloud.cn/platform.php?vt=1

http://news.newhua.com/news/2011/1206/138541.shtml

http://www.google.org

http://www.cnnic.cn/research/bgxz/ydhwbg/201108/t2011082922658.html

马志欣,王宏,李鑫. 语音识别技术综述[J].昌吉学院学报,2006(3):93-97.

吕云芳. 基于模板匹配法的语音识别系统研究与基本实现[D]. 河北工业大学, 2005.

聂晓飞, 赵禹, 詹庆才. 一种基于模板匹配的语音识别算法[J]. 电子设计工程, 2011, 19(19):58-60.

范崇山, 陈新伟, 罗智荣,等. 典型简单模板匹配语音识别方式技术研究[J]. 科技视界, 2017(7):238-239.

潘智刚, 姚敏锋, 张晶. 多模板优化的语音识别算法[J]. 电脑知识与技术, 2015(1):146-149.

李琼. 基于信号与内容特征的音频判决系统的 研究与实现[D]. 电子科技大学, 2015.

黄瑞. 面向移动终端的语音助手AudioPhone的设计与开发[D]. 浙江大学, 2012.

袁俊. HMM连续语音识别中Viterbi算法的优化及应用[J]. 电子技术, 2001, 28(2):46-49.

王作英. 非齐次语音识别HMM模型和THED语音识别与理解系统[C]// 全国人机语音通讯学术会议. 1992:31-954.

张立朋, 李立梅. 一种用于语音识别的神经网络[J]. 北京邮电大学学报, 1995(1):31-37.

游小微. 语音识别的神经网络方法研究[J]. 浙江师范大学学报(自然科学版), 2002, 25(3):255-257.

Waibel A. Phoneme recognition: Neural network vs. Hidden Markov models[J]. proceeding from ICASSP, 1988(13):78- 85.

Zhang L X, Xia A G, Zhao D L.Predicting chains dimesions from an artificial neural network model[ J].J Polym Sci; Polym Phys.(PartB), 2000, 38: 3 163- 3 167.

孙淑娟, 牟德昆. 软插件模式在Windows语音助手中的应用[J]. 潍坊学院学报, 2011, 11(4):27-31.