人工智能综述报告













   

课程名称：人工智能原理与方法

任课老师：张重阳

课题：语音助手的研究及产业应用情况

            学院：计算机科学与工程

            姓名：顾一凡

            学号：117106021901















日期：2017年11月21日



语音助手的研究及产业应用情况

研究的主要内容和意义

以自然语言人机交互为主要目标的自动语音识别（ASR），在近几十年来一直是研究的热点，通过智能对话与即时问答的智能交互，实现帮忙用户解决问题，其主要是帮忙用户解决生活类问题，苹果手机中siri开创智能语音助手的先河，当然中文语音助手也如雨后春笋般蓬勃发展，在中文方面的智能搜索程度依然超过siri。众多研究者将Geoffrey Hinton在2006年发表的深度置信网络的论文视为深度学习出现的标志，但那时，该技术还只是多层深度置信网络权值初始化的一种有效理论尝试，仅仅对一小部分机器学习产生影响。真正让深度学习蓬勃发展的，是其在应用领域的巨大成功。而语音识别正是深度学习取得成功的应用领域之一。

语音识别的发展自20世纪70年代采用隐马尔科夫模型（HMM）进行声学建模以来，每个时代都有经典的创新成果。深度学习技术在21世纪第二个十年产生最重大的影响，就是使得语音识别错误率在以往最好的基础上相对下降30％或者更多，而这一下降刚好突破了语音识别真正可用的临界点。该技术的突破伴随着并行计算基础设施的发展和移动互联网大数据的产生，其影响进一步扩大，目前已经成为业界无争议的标准前沿技术。

语音助手的作用：

生活帮助：打电话、发短息、周边查询、公交线路、火车车次、航班查询、天气预报、应用下载、软件管理、生活备忘、百科知识、网址导航、知识搜索、语音翻译、歌曲播放、邮编查询、航班、快递、电话归属地查询、地图导航、智能计算、彩票查询等等。 

休闲娱乐：讲笑话、查看新闻。 

智能聊天：日常会话、情感倾诉、调侃挑逗。



国内外研究状况和发展水平

在2000年以前，有众多语音识别相关的核心技术涌现出来，例如：混合高斯模型、隐马尔科夫模型、梅尔倒谱系数及其差分、ASR以及相关领域的发展。但是比较起来，在2000到2010年间，虽然GMM-HMM序列鉴别性训练这种重要的技术被成功应用到实际领域中，但是语音识别领域中无论是理论研究还是实际应用，进展都相对平淡而缓慢。

然而在过去的几年里，语音识别领域的领域的研究领域的研究热情又一次被点燃。由于移动设备对语音识别的需求与日俱增，并且众多新型语音应用，例如，语音搜索、短信听写、虚拟语音助手等在移动互联网世界取得成功，新一轮热潮带动起来。此外，由于计算能力的显著提升和大数据的驱动，深度学习在大词汇连续语音识别的成功应用也是同样重要的影响因素。比起此前最先进的识别技术，GMM-HMM框架，深度学习在众多真实世界的大词汇连续语音识别任务中都使得识别的错误率降低了三分之一或者更多，识别率也进入到真实用户可以接受的范围内。以下举三个例子：









苹果Siri 





苹果手机中国用户1.2亿，30%使用者。siri公司成立于2007年，2010年被苹果收购，并在2011年于苹果手机上线。最初以文字聊天服务为主，随后通过与全球最大的语音识别厂商nuance合作，siri实现了语音识别功能。使用者可以通过声控、文字输入的方式，来搜寻餐厅、电影院等生活信息，同时也可以直接收看各项相关评论，甚至是直接订位、订票；另外其适地性（location based）服务的能力也相当强悍，能够依据用户默认的居家地址或是所在位置来判断、过滤搜寻的结果。发布之初，支持语言种类少、反应缓慢，语义理解能力差，人们好奇一时兴起，踊跃尝试功能。时间推移，siri增加更多功能，提升应用性能，与电话、地图、提醒、音乐等功能实现连接，适当时候为用户处理一些简单事情。应用在apple watch上，常被使用功能：创建提醒、设置计时器等。2016年ios10系统中，苹果向第三方应用软件开发商开放功能接口，可让应用接入siri，一些第三方应用第一时间支持siri功能，比如QQ、滴滴出行等，虽然已经实现支持，但实际使用中，表现不尽如人意。十一功用：1. 闹钟 2. 找咖啡厅 3. 找路线，ios 10中，易到支持siri语音叫车并帮助完成订车4. 随机播放音乐5. 发送短信6. 天气预报7. 提醒日程安排. 提醒地点9. 答疑解惑10. 发微博，新浪微博+腾讯微博11. 订电影票（美国）。Iphone7 蓝牙无限耳机，轻轻拍击二次耳机来启动siri助手，实现与siri进行交互。



谷歌assistant





2008年，谷歌首次推出语音助手，2012年为其建立了知识图谱，允许你搜索Google所知道的人、事、物，使搜索智能化。而今推出google assistant，拥有情境感知能力，不仅能够理解问题的相关情境，还能分析问题背后真正的意图。

Google Assistant并不是一种单独的程序，这款技术会和谷歌不同的设备以及操作系统机密结合，基于android平台的手机、汽车、手表等都可以使用此技术。也能在第三方应用和服务中使用，如媒体音乐服务、打车服务等。旨在让用户通过“流畅”的语音和设备相互沟通。与微软小娜服务接近，基于语音和文字搜索，支持用户和机器双向连续对话，人工智能程序可以自动根据用户的个性化需求来实时反馈结果。比如用户给出指令，看电影，谷歌助理可直接帮助完成订票操作。

Assistant具体应用：谷歌在16年发布了搭载assistant的一款硬件google home和一款IM软件allo。Google home，具象化的google assistant，作为未来google智能家居的入口。能调用谷歌搜索以及其他应用程序，按照用户语音指令执行播放音乐、关闭房间照明、回答知识性问题、查询交通状况、帮用户更改预约等任务。

     

微软小娜





2014年8月中国上线。15年跨平台。可以通过设置提醒，来更好地管理你每日的忙碌行程，通过语音来直接搜索，进入你想去的网站，查询像航班信息这样重要的消息，也可以跨平台地对任务进行处理。以下是Cortana（小娜）功能以及相应叙述：聊天功能：讲一个笑话、成语接龙、讲一个故事、唱一首歌、模仿宋小宝。通讯功能：给妈妈打电话、给爸爸发短信。提醒功能：提醒我12:00去舅舅家、将下午12:00的日程更改到18:00。娱乐功能：播放音乐、今日热映、《红楼梦》、名人微博。交通功能：我在哪里、怎么去广场、附近餐馆、今日限行尾号。查询功能：今日天气、澳航航班会晚点吗、使用英语翻译我的名字 、世界上陆地面积最大的国家、今年春节 放假安排、今日资讯、双色球、大乐透。召唤小冰：召唤小冰。必应美图：必应美图。

微软小娜是微软发布的全球第一款个人智能助理。它“能够了解用户的喜好和习惯”，“帮助用户进行日程安排、问题回答等”。 cortana 可以说是微软在机器学习和人工智能领域方面的尝试。微软想实现的事情是，手机用户与小娜的智能交互，不是简单地基于存储式的问答，而是对话。它会 记录用户的行为和使用习惯，利用云计算、搜索引擎和“非结构化数据”分析，读取和“学习”包括手机中的文本文件、电子邮件、图片、视频等数据，来理解用户 的语义和语境，从而实现人机交互。 一个很简单的例子就是，假如手机中记录的日程显示将要参加会议，那么不需任何操作，Cortana 到时就会自动将手机调至会议状态。



算法原理、技术路线的分析比较

传统声学模型

当使用混合高斯随机变量的分布用于匹配语音特征时，就形成了混合高斯模型（GMM）。 

1.1随机变量 

1）随机变量可以理解为从随机实验到变量的一个映射； 

2）随机变量所有可能的取值成为域； 

3）连续随机变量的概率密度函数（Probability density function，PDF）：p（x） 

4）随机变量在x=a处的累计分布函数 ： 



 

1.2 高斯分布和混合高斯随机变量 

1）高斯分布 

标量：若随机变量x的概率密度函数是： 



 

那么它服从正态分布或高斯分布，记为： 



 

矢量/向量：对于高斯分布的随机变量矢量x=（x1,x2,…,xD）T,也成为多元或向量值高斯随机变量，其联合概率密度函数为： 



同样，记为： （均值，协方差） 

2）混合高斯分布 

标量：服从混合高斯分布的随机变量x，其概率密度函数为：（M个高斯）

 

 

其中混合权重为正实数，其和为1： 

混合高斯分布最明显的性质是他的多模态（M大于1），不同于高斯分布的单模态（M=1），这使得混合高斯模型足以描述很多显示出多模态性质的物理数据，比如语音数据，而单高斯分布则不合适。 

矢量/向量：多变量的多元混合高斯分布，即，服从高斯分布的随机变量向量，其联合概率密度函数为： 



 

在DNN出现之前，GMM是提升语音识别系统性能的一个关键，且一般M为一个先验值。 

如果变量x的维度D很大，比如语音识别中的特征40维，那么使用全协方差矩阵（非对角）（∑m）将引入大量参数（大约M x D^2）。为了减少这个数量，可以使用对角协方差矩阵。对角协方差矩阵极大简化了计算量。将全协方差矩阵近似为对角协方差矩阵看似是使用了各维度不相关的假设，实际是一种误导，因为，混合高斯模型具有多个高斯成分，虽然每个成分都使用了对角协方差矩阵，但总体上至少可以有效的描述由一个使用全协方差矩阵的单高斯模型所描述的向量维度相关性。



	隐马尔可夫模型及其变体

隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。

是在被建模的系统被认为是一个马尔可夫过程与未观测到的（隐藏的）的状态的统计马尔可夫模型。

下面用一个简单的例子来阐述：

假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。







假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8

一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。

同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。









其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。

　但在序列某个地方有噪声干扰的时候，某些方法可能会和正确答案相差的较远。但是 Viterbi 算法会查看整个序列来决定最可能的终止状态，然后通过后向指针来找到之前的状态，这对忽略孤立的噪声非常有用。

　　Viterbi 算法提供了一个根据可观察序列计算隐藏序列的很高效的方法，它利用递归来降低计算复杂度，并且使用之前全部的序列来做判断，可以很好的容忍噪声。

　　在计算的过程中，这个算法计算每一个时刻每一个状态的部分概率，并且使用一个后向指针来记录达到当前状态的最大可能的上一个状态。最后，最可能的终止状态就是隐藏序列的最后一个状态，然后通过后向指针来查找整个序列的全部状态。

　　 根据观察到的序列集来找到一个最有可能的 HMM。 

　　在很多实际的情况下，HMM 不能被直接的判断，这就变成了一个学习问题，因为对于给定的可观察状态序列 O 来说，没有任何一种方法可以精确地找到一组最优的 HMM 参数 λ 使 P(O | λ) 最大，于是人们寻求使其局部最优的解决办法，而前向后向算法（也称为Baum-Welch算法）就成了 HMM 学习问题的一个近似的解决方法。

　　前向后向算法首先对于 HMM 的参数进行一个初始的估计，但这个很可能是一个错误的猜测，然后通过对于给定的数据评估这些参数的的有效性并减少它们所引起的错误来更新 HMM 参数，使得和给定的训练数据的误差变小，这其实是机器学习中的梯度下降的思想。

　　对于网格中的每一个状态，前向后向算法既计算到达此状态的“前向”概率，又计算生成此模型最终状态的“后向”概率，这些概率都可以通过前面的介绍利用递归进行高效计算。可以通过利用近似的 HMM 模型参数来提高这些中间概率从而进行调整，而这些调整又形成了前向后向算法迭代的基础。



	深度神经网络中的特征表示学习     

 2006年，Hinton提出深度学习网络，指出深度神经网络因为层数过多导致训练参数多的问题可以利用逐层初始化解决。在工业界和学术界掀起了深度学习的浪潮，并在语音识别和图像处理领域取得了巨大成功。2011年微软和谷歌在语音识别上采用DNN模型，将词错误率降低20%-30%。这里的DNN主要采用的是DBN，即深度置信网络。随着语音识别以及深度学习的发展，研究人员发现将CNN和RNN模型应用于语音识别领域可以取得更好的效果。本文中主要介绍声学模型的优缺点。

    CNN模型，即卷积神经网络，最开始应用于图像处理。而语音识别中的频谱图，应用于CNN模型，可以克服传统语音识别中采用时间、频率而导致的不稳定问题。DBN和CNN模型没有考虑语音之间的关联信息。而RNN模型，充分考虑了语音之间的相互关系，因此取得更加好的效果。现有的最好的基于深度学习的语音识别一般是基于DBN+CNN+RNN模型的。

上述总结成为，现有的声学模型建立，一般可分为：

（1）混合声学模型

混合高斯-隐马尔科夫模型   GMM-HMM

就用多维的高斯模型区模拟我们得到的矩阵，相当于拟合。相当于我们假设其服从高斯分布，然后我们寻找均值和方差矩阵。可以理解为下图。



我们可以看到特征后用一个多维高斯函数去模拟。此外，这里还有个重要的东西，首先我们在训练阶段，我们是知道这段语音所表示的句子吧。我们通过句子，然后分词，然后分成每个音素，在隐马尔科夫（HMM）模型中一般用3-5个上述的单元表示一个音素。简单的理解就是我们每个音素的均值和方差矩阵知道，通过我们的句子我们也知道每个音素间的转移概率矩阵。当然，这些是HMM里的事情。提取特征后的第一步就完成了，简单的说就是为了拟合多维高斯函数。

深度神经网络-隐马尔科夫模型   DNN-HMM

    与GMM-HMM系统的区别是分类器的使用。在DNN-HMM混合系统中的隐层与输出层链接很稀疏的单隐层神经网络建模。从另一个角度来说，因为隐层和输出层的对数线性分类器的训练是同时优化的，在DNN-HMM混合系统中的隐层特征与分类器的匹配会比Tandem和瓶颈特征中更好。

深度循环神经网络-隐马尔科夫模型   RNN-HMM

深度卷积神经网络-隐马尔科夫模型   CNN-HMM

端到端的声学模型





连接时序分类-长传统的语音识别需要把语音转换成语音特征向量，然后把这组向量通过机器学习，分类到各种音节上（根据语言模型），然后通过音节，还原出最大概率的语音原本要表达的单词，一般包括以下模块：

特征提取模块 (Feature Extraction)：该模块的主要任务是从输入信号中提取特征，供声学模型处理。一般也包括了一些信号处理技术，尽可能降低环境噪声、说话人等因素对特征造成的影响，把语音变成向量。





声学模型 (Acoustic Model): 用于识别语音向量

发音词典 (Pronnuciation Dictionary)：发音词典包含系统所能处理的词汇集及其发音。发音词典提供了声学模型与语言模型间的联系。

语言模型 (Language Model)：语言模型对系统所针对的语言进行建模。

解码器 (Decoder)：任务是对输入的信号，根据声学、语言模型及词典，寻找能够以最大概率输出该信号的词串。

传统的语音识别中的语音模型和语言模型是分别训练的，缺点是不一定能够总体上提高识别率。短时记忆模型CTC-LSTM

注意力模型Attention      

各个模型的优缺点介绍

（1）基于GMM-HMM的声学模型 

优点：GMM训练速度快

           声学模型较小，容易移植到嵌入式平台

缺点：GMM没有利用帧的上下文信息

            GMM不能学习深层非线性特征变换

（2）基于DNN-HMM模型

优点： DNN能利用帧的上下文信息，比如前后个扩展5帧

             DNN能学习深层非线性特征变换，表现优于GMM

缺点： 不能利用历史信息来辅助当前任务

（3）基于RNN-HMM模型：

优点：  RNN能有效利用历史信息，将历史消息持久化

              在很多任务上，RNN性能变现优于DNN

缺点：  RNN随着层数的增加，会导致梯度爆炸或者梯度消失

（4）基于CNN-HMM声学模型

优点：CNN对于语音信号，采用时间延迟卷积神经网络可以很好地对信号进行描述学习

CNN比其他神经网络更能捕捉到特征的不变形接下来将详细介绍各个声学模型的应用，现在比较熟悉的是DBN模型以及CNN模型，而RNN模型还在学习中。 



其他研究和应用情况

1，循环神经网络及相关模型  

   RNNs的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关

   CW-RNNs是较新的一种RNNs模型，其论文发表于2014年Beijing ICML。在原文[8]中作者表示其效果较SRN与LSTMs都好。 

  CW-RNNs也是一个RNNs的改良版本，是一种使用时钟频率来驱动的RNNs。它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。并且为了降低标准的RNNs的复杂性，CW-RNNs减少了参数的数目，提高了网络性能，加速了网络的训练。CW-RNNs通过不同的隐藏层模块工作在不同的时钟频率下来解决长时间依赖问题。将时钟时间进行离散化，然后在不同的时间点，不同的隐藏层组在工作。因此，所有的隐藏层组在每一步不会都同时工作，这样便会加快网络的训练。并且，时钟周期小的组的神经元的不会连接到时钟周期大的组的神经元，只会周期大的连接到周期小的(认为组与组之间的连接是有向的就好了，代表信息的传递是有向的)，周期大的速度慢，周期小的速度快，那么便是速度慢的连速度快的，反之则不成立。CW-RNNs与SRNs网络结构类似，也包括输入层(Input)、隐藏层(Hidden)、输出层(Output)，它们之间也有向前连接，输入层到隐藏层的连接，隐藏层到输出层的连接。但是与SRN不同的是，隐藏层中的神经元会被划分为若干个组，设为g，每一组中的神经元个数相同，设为k，并为每一个组分配一个时钟周期T

i∈{T1,T2,...,Tg}，每一个组中的所有神经元都是全连接，但是组j到组i的循环连接则需要满足Tj大于Ti。如下图所示，将这些组按照时钟周期递增从左到右进行排序，即

T1<T2<...<Tg，那么连接便是从右到左。例如：隐藏层共有256个节点，分为四组，周期分别是[1,2,4,8]，那么每个隐藏层组256/4=64个节点，第一组隐藏层与隐藏层的连接矩阵为64*64的矩阵，第二层的矩阵则为64*128矩阵，第三组为64*(3*64)=64*192矩阵，第四组为64*(4*64)=64*256矩阵。这就解释了上一段的后面部分，速度慢的组连到速度快的组，反之则不成立。 

  CW-RNNs的网络结构如下图所示： 

 

  在传统的RNN中，按照下面的公式进行计算： 

st=fs(Wst−1+Winxt)

ot=fo(Woutst)



  其中，W为隐藏层神经元的自连接矩阵，Win为输入层到隐藏层的连接权值矩阵，Wou是隐藏层到输出层的连接权值矩阵 ，xt是第t步的输入，st−1为第t−1步隐藏层的输出，st为第t步隐藏层的输出，ot为第t步的输出，fs为隐藏层的激活数，

fo为输出层的激活函数。 与传统的RNNs不同的是，在第t步时，只有那些满足(tmodTi)=0的隐藏层组才会执行。并且每一隐藏层组的周期{T1,T2,...,Tg}都可以是任意的。原文中是选择指数序列作为它们的周期，即Ti=2i−1i∈[1,...,g]。 

因此W与Wi将被划分为g个块。如下： 



其中是一个上三角矩阵，每一个组行Wi被划分为列向量{W1i,...,Wii,0(i+1)i,...,0gi}T，Wji,j∈[1,...,g]表示第i个组到第j个组的连接权值矩阵。在每一步中，W与Win只有部分组行处于执行状态，其它的为0：




  为了使表达不混淆，将Win写成Win。并且执行的组所对应的o才会有输出。处于非执行状态下的隐藏层组仍保留着上一步的状态。下图是含五个隐藏层组在

t=6

时的计算图： 



 

  在CW-RNNs中，慢速组(周期大的组)处理、保留、输出长依赖信息，而快速组则会进行更新。CW-RNNs的误差后向传播也和传统的RNNs类似，只是误差只在处于执行状态的隐藏层组进行传播，而非执行状态的隐藏层组也复制其连接的前面的隐藏层组的后向传播。即执行态的隐藏层组的误差后向传播的信息不仅来自与输出层，并且来自与其连接到的左边的隐藏层组的后向传播信息，而非执行态的后向传播信息只来自于其连接到的左边的隐藏层组的后向传播数据。 

卷积神经网络

BP浅层神经网络早在20世纪80年代末期就已掀起了基于统计模型的机器学习热潮，只是BP只有一个隐层，人们对于bp没有很完整的理论推导而且调参问题不少，这样深度神经网就被搁置了。相比而言，20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、 Boosting、最大熵方法（如LR，Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。

直到2006年，加拿大多伦多大学教授、机器学习领域的泰斗Geoffrey Hinton和他的学生RuslanSalakhutdinov在《科学》上发表了一篇文章，开启了深度学习在学术界和工业界的浪潮。这篇文章有两个主要观点：1）多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2）深度神经网络在训练上的难度，可以通过“逐层初始化”（layer-wise pre-training）来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。（摘抄自xy哥哥那里，哈）

2012年神经网络又开始崭露头角，那一年Alex

    Krizhevskyj在ImageNet竞赛上（ImageNet可以算是竞赛计算机视觉领域一年一度的“奥运会”竞赛）将分类错误记录从26%降低到15%，这在当时是一个相当惊人的进步。从那时起许多公司开始将深度学习应用在他们的核心服务上，如Facebook将神经网络应用到他们的自动标注算法中，Google（谷歌）将其应用到图片搜索里，Amazon（亚马逊）将其应用到产品推荐服务，Pinterest将其应用到主页个性化信息流中，Instagram也将深度学习应用到它们的图像搜索架构中。

深度学习作为人工智能的一个方向，迅速风靡。

2011 Google Brain(内部共有10亿个节点,人脑中可是有150多亿个神经元)

2012 Google 实验室,百度研究院

2014 Facebook的DeepFace

2015 亚马逊发布了自己的机器学习平台；微软创分布式机器学习工具包

2016 谷歌人工智能算法 AlphaGo (1959年美国的塞缪尔(Samuel)就已经设计了一个下棋程序)

 受Hubel和Wiesel对猫视觉皮层电生理研究启发，有人提出卷积神经网络（CNN），据说是小猫头颅开了个小孔，然后不断在其眼前闪过不同类型、不同强弱的刺激。然后测试大脑视觉皮层细胞的响应，最后发现只有部分细胞响应。David Hubel 和Torsten Wiesel 发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。

这个启发了深度神经网络，比如我们要识别出图片中的小猫，并不需要像之前的全连接的结构，因为大量图片物体识别起来参数会相当多。我们在第一层卷积层得到的是各物体边缘特征，（接着pooling），第二个卷积层可能就得到边缘的组合或者小猫的部位碎片，...到最后就会得到诸如爪子呀，眼睛之类的高层特征啦。

    所谓：仿视觉皮层（多层），局部特征组合与抽象。Yann Lecun 最早将CNN用于手写数字识别并一直保持了其在该问题的霸主地位。近年来卷积神经网络在多个方向持续发力，在语音识别、人脸识别、通用物体识别、运动分析、自然语言处理甚至脑电波分析方面均有突破。

     卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。在卷积神经网络的卷积层中，一个神经元只与部分邻层神经元连接。在CNN的一个卷积层中，通常包含若干个特征平面(featureMap)，每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。共享权值（卷积核）带来的直接好处是减少网络各层之间的连接，同时又降低了过拟合的风险。子采样也叫做池化（pooling），通常有均值子采样（mean pooling）和最大值子采样（max pooling）两种形式。子采样可以看作一种特殊的卷积过程。卷积和子采样大大简化了模型复杂度，减少了模型的参数。 

语音助手的总结及未来方向展望

     通过结合CNN、DNN和基于i-vector的自适应技术，IBM的研究人员在2014年展示他们能将Switchboard Hub5评估集的词错误率降到10.4％。对比最好的GMM系统在同样的测试集上14.5％的错误率，DNN的错误率大大缩减。这只是通过声学模型改进达到的。语音识别在有些情况下表现依然很糟糕：远场麦克风语音识别，高噪声环境下语音识别，带口音的语音识别，多人语音背景下的语音识别，不流利的自然语音，这些情况。需要新的技术进展或者更精巧的工程设计将错误率降到最低。

我们相信，语音识别中即使没有用到声学模型部分出现大量新技术，但语音识别的准确率在上述一些条件的情况下还是能够提高的，例如，使用更先进的麦克风列阵技术，我们可以更显著地降低噪音和背景交谈的影响，从而在这些条件下提高语音识别的准确率。最近，词嵌入的概念被引入语音识别系统，作为传统的基于音素的词典模型的替代，并提高了识别准确率，这例证了一种新方法，即基于连续向量空间的分布表示来给语言符号建模，将其作为识别输出。

更长远地看，我们相信语言识别研究能从人脑研究项目以及特征表示的编码和学习，具有长程依赖和条件状态转移的循环网络，多任务和无监督学习这些领域中获益，同时研究人员需要看到其他领域，如认知科学，计算语言科学，知识表示和管理，人工智能以及神经科学。