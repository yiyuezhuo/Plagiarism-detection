语音识别四种分类方法的综述
1.研究的主要内容和意义
移动互联网已迅速成为当今发展最快、规模最大和市场前景最好的行业之一，
己吸引了众多知名公司和科研机构进军该领域。由于现有的移动终端设备交互方
式存在很多的局限，如键盘输入文字不便，操作繁琐等，而语音是人机交互最自
然的方式，近些年来，正在日益影响和改变人们的日常生活。移动互联网对语音
技术本身有着天然的需求，随着移动互联时代的到来，移动终端作为人手必备的
工具，语音技术带来的交互优势更加明显，可以很大程度地提高移动终端交互效
率和改善交互体验。语音技术包括语音识别和语音合成技术，这两个技术使得移
动终端具备了能听会说的能力。
让计算机能听懂人类的语言，是人类自计算机诞生以来梦寐以求的愿望。随
着计算机越来越向便携化方向发展，随着计算环境的日趋复杂，人们越来越迫切
地要求摆脱键盘的束缚而代之以语音输入这样简便使用的、自然的、人性化的输
[1]
入方式 。作为高科技应用领域的研究热点，语音识别(Speech Recognition)技
术从理论的研究到产品的开发已经走过了四十多个春秋并且取得了长足的进步，
它正在直接与办公或商业系统的数据库语音查询、数据库维护与管理、语音输入、
工业生产部门的语声控制(Command＆Contr01)、电话与电信系统的自动拨号、辅
助控制与查询以及医疗与卫生部门的专业报告的语音创建与编辑等各种实际应
用相接轨，并且极有可能成为下一代操作系统和应用程序的用户界面。据统计，
全世界的语音识别市场正在以每年 20％的速度飞速增长。与此同时，目前的语
音识别技术研究水平还远不能达到使计算机与人类之间能够自然交流的这个终
极目标，甚至曾有专家比喻其难度要超过“人类登上月球”!可见，语音识别技
术的研究将是一项极具市场价值和挑战性的工作。我们今天进行这一领域的研究
与开拓就是要让语音识别技术走入人们的日常生活当中，并不断朝更高目标而努
力。
2.国内外研究现状和发展水平
本节介绍语音识别在国内外的发展，主要有科大讯飞、苹果公司手机 Siri
个人助手、谷歌语音搜索与 Voice Actions、微软语音搜索软件 Tellme。
[2]
科大讯飞的 MSP(I FLY Mobile Speech Platform) 。MSP 是一个应用于移动
互联网的语音服务平台，该平台提供了架构于互联网的语音云服务和一套移动互
联网语音解决方案、应用示例，把语音服务的应用范围拓宽到移动互联网领域，
为语音服务产品走向移动互联网市场开辟的应用模式。MSP 平台整合了科大讯飞
研究院、中国科技大学讯飞语音实验室以及清华大学讯飞语音实验室在语音识别、
语音合成等技术上多年的技术成果，采用分布式架构，继承了科大讯飞电信级语
音平台高稳定的特点。该系统的特点是综合了很多语音处理的相关技术，包括语
音识别，说话人识别，语音合成等，为语音技术的应用和推广提供很好的实例和
平台，但所有核心算法都是在服务器上运行，用户必须通过网络连接到该平台的
服务器上，服务器进行计算后将结果传送给用户。该模式识别速度会受到网络状
况的限制，这给用户带来很多的不便。
[3]
苹果公司手机 Siri 个人助手 。Siri 技术来源于美国国防部高级研究规划
局所公布的 CALO 计划：一个让军方简化处理一些繁复事物，并具有学习、组织
以及认知能力的数字助理。Siri 则是其所衍生出来的民用版软件。Siri 通过语
音合成技术来读短信、介绍餐厅、询问天气、语音设置闹钟等；通过语音识别技

术来调用系统自带的天气预报、日程安排、搜索资料等基本应用，还能够不断学
习新的声音和语调，提供智能的对话式应答。该应用软件特点是友好的用户界面
和体验，交互式问答模式。
[4]
谷歌语音搜索与 Voice Actions 。该语音搜索引擎支持英文、中文以及中
英混合的语音输入，并采用了噪音分离技术，从一定程度上降低了背景噪音的影
响。Voice Actions 是谷歌于 2010 年 8 月推出的一款应用，利用该应用可以通
过语音命令控制内置应用软件，搜索地理位置。虽然 Voice Actions 提供了基本
的语音识别引擎，不过 Voice Actions 要求语音输入符合严格的语法结构，否则
系统将无法识别，系统鲁棒性较差。
[5]
微软语音搜索软件 TelIme 。TelIme 语音识别引擎采用云计算模式，并能通
过说话人语音自适应技术，使得识别率在使用过程中得到不断提升。2010 年微
软将 Tellme 整合进 Windows Phone 7，提供两种基本操作：拨打电话给联系人
和启动应用程序。微软通过苹果 Appstore 商店发布了 iPad 版必应，在必应 iPad
版本中微软加入了语音搜索功能。
3. 语音识别四种分类方法的比较研究
语音识别系统模型通常由声学模型和语言模型两部分组成。声学模型能否真
实地反映话音的物理变化规律, 语言模型能否表达自然语言所包含的丰富语言
学知识,是语音识别系统性能好坏的关键。然而语音信号和自然语言都是随机多
变和不稳定的,这是目前语音识别中最大的难点。
声学模型是识别系统的底层模型,其目的是提供一种计算语音的特征矢量序
列和每个发音模板之间距离的方法。人的发音在每一时刻都受到其前后发音的影
响，为了模仿自然连续语音中协同发音作用和鉴别这些不同发音,通常要求使用
复杂的声学模型。声学模型的设计和语言发音特点密切相关。声学模型单元大小
(字发音模型、半音节模型或音素模型)对语音训练数据量大小、系统识别率、以
及灵活性有较大的影响。对大词汇量语音识别系统来说，通常识别单元小，则计
算量也小，所需的模型存储量也小，要求的训练数据量也少。但带来的问题是对
应语音段的定位和分割较困难,识别模型规则也变得更复杂。通常大的识别单元
在模型中包括协同发音，这有利于提高系统的识别率，但要求的训练数据相对增
加。因此，识别单元的大小应根据语言的特点、识别系统词汇量的大小而定。
主要的语音识别分类有 4 种：样本匹配法、判决系统、HMM 声学建模和神经
[6]
网络 。
3.1 样本匹配法
[7]
一个典型的样本匹配过程 ：未知(待识别)语音经过话筒变换成电信号，后
加在识别系统的输入端，首先要经过预处理，预处理包括去噪、预加重、分帧、
端点检测、加窗。经过预处理后的语音信号再经过特征提取(特征提取包括自相
关分析、LPC 分析及倒谱分析)，语音信号的特征被提取出来。常用的特征包括：
短时平均能量或幅度、短时平均过零率、短时自相关函数、线性预测系数、倒谱、
共振峰等。根据实际需要选择语音特征参数(本课题选择基于 LPC 的倒谱特征)，
这些特征参数的时间序列便构成了待识别语音的模式，将其与已经存储在计算机
内的参考模式逐一进行比较(模式匹配 1，获得最佳匹配(由判决规则确定)的参
考模式便是识别结果。参考模式是在系统使用前获得势存储起来的，为此，要输
入一系列已知语音信号，提取它们的特征作为参考模式，这一过程称为训练过程。
这个过程可以用图 1 进行表示。

图 1 基于模式匹配原理的语音识别系统的原理框图
训练过程可概括为预处理、声音截取和分帧、帧处理得到数据文件和模板的
[8]
匹配判断 。
3.1.1 预处理
首先是对声音信号的预处理。预处理第 1 步是采样，按照 8 kHz 的采样频率
进行直接采集。根据奈奎斯特定理，这个采样频率可以保证声音信号的无失真复
原。第 2 步是对声音信号幅值的一个削弱，因为声音信号由麦克输入，声音幅值
会很大，造成不必要的干扰。第 3 步是加窗，加汉明窗，滤掉高频成分，可以有
效防止频谱的混叠。由于用麦克输入的声音信号的信噪比都比较高，所以就不用
考虑滤波这个环节了。预处理这个环节就是为了给后面的各个模块提供品质较好
的语音信号数据。
3.1.2 声音截取和分帧
接下来就是端点的检测。语音信号经过预处理后，要提取出字词，就必须通
过进行端点检测来去除前后两端无声区的影响，使得声音信号尽可能不受人为输
入反应时间的干扰。第一步是计算所有声音信号幅值的平均值 a，当声音的幅值
达到平均值的 1 a ，我们就认为这个字开始了，当声音信号再次下降到平均值
2
的 1 a ，我们就认为这个字结束了。在这里，幅值代替了功率的作用， 1 a 和
3
2
1 a 作为判断的两个阈值。因为说话的声音的幅值肯定大于背景噪声，所以这
3
个方法在安静环境下或者采用麦克风输入时效果还可以接受。如果环境质量不高，
还可以结合过零率等参数来进行端点检测。
3.1.3 帧处理得到数据文件
帧处理的过程如下：
第 1 步，对语音信号每一帧的加窗语音做 256 点的 DFT 变换，得到频域上的
表达式，即功率谱。
第 2 步，划分临界带。
第 3 步，求临界带特征矢量。目前特征是语音信号预处理中的重要步骤。在
实际特征提取中，较常采用的参数是线性预测倒谱系数（LPCC）和 Mel 倒谱系数
（MFCC）[9]。二者采用的均是时域转换到倒谱域上，但是出发思路两者不同。线
性预测倒谱系数（LPCC）以人类发声模型为基础，采用线性预测编码（LPC）技
术求倒谱系数；Mel 倒谱系数（MFCC）以人类听觉模型为基础，通过离散傅利叶
变换（DFT）进行变换分析。
3.1.4 模板的匹配判断
比较语音参量我们用的是动态时间规整（Dynamic Time Warping，DTW）算
法，DTW 算法能够较好地解决用于孤立词识别时说话速度不均匀的难题。测试的
语音参数共有 N 帧矢量，而参考模板有 M 帧矢量，且 M 不等于 N，则 DTW 就是寻

找一个时间归整函数，它将测试矢量的时间轴 n 非线性地映射到模板的时间轴 m
上，并使该函数满足第 n 帧测试矢量和第 m 帧模板矢量之间的距离测度最小。
[10]
在文献 中，提出一个多模板的优化思想。取两个模板为一个小组，采用动
态规整的方法得到两模板的匹配路径。然后两模板根据匹配路径，让对应帧的特
征参数相加之后取平均得到一个新模板。然后所有小组产生的新模板相加取平均，
得到优化模板。这个优化模板与三个原始模板的都有很强的相关性，这种相关性
与个人的发音本质相对应，原始模板与优化是一般性与特殊性的关系。这样一来，
优化模板就很好地结合了多模板参与匹配的人性化思想，同时优化模板只有一个，
又有了单模板匹配的简洁快速的特性。
显然，最佳匹配结果的获得与特征的选择、语音模型的好坏、模板是否准确
都有直接的关系，这也是目前语音识别过程中的一个难点。
3.2 判决系统
[11]
音频判决系统 从音频信号和内容特征的两个方面入手，解决音频质量检测
与内容检测两大问题。基于信号特征的音频判决是指系统根据不同用户的需求来
设置相应的音频质量检测参数选项，通过计算机来对待检测的音频文件进行质量
检测。这种判决方式不需要工作人员过多的参与，而是通过计算机来操作，但是
前提是需要给出合理的算法才能达到不错的检测水平。实验室播出系统有一个对
音频质量进行检测的模块，没有对音频的内容进行审核，本论文的工作主要是在
完成基于信号的音频判决模块的前提下，同时提出自己的一个创新点，设计并实
现出基于内容特征的音频判决模块，从而完善整个音频判决系统。基于内容特征
的音频判决是指通过语音识别及关键字识别相关算法实现对一段音频的内容进
行甄别，判断音频的内容是否有敏感字眼，诸如“法轮功”等，这对广播节目的
安全播出尤为关键。

图 2 音频判决系统的设计
如上图 2，音频判决系统的音频审核部分是一系列识别检测过程，主要针对
音频采样数据进行分析。一方面，如果用户选择音频质量检测，可以检测诸如静
音、立体声相位反相等异常问题。另一方面，如果用户选择音频内容检测，系统
先进行语音识别，将语音转化为文本，然后基于文本结果进行关键词搜索，最后
对检索到的关键词进行定位。音频质量检测和音频内容检测之间是并联关系，相
互独立，用户根据自己的需求选择检测项。另外音频质量检测部分的内部实现子
检测项之间是相互独立的，没有一定的先后顺序。相反，音频内容检测部分的内
部实现子检测项之间是串联关系，必须遵循一定的先后顺序。该系统能够支持多

种格式的音频文件，对文件的检测能够精确到每帧，用户根据自己的需要来选择
具体的检测项，系统的最终检测结果在软件界面上显示出来。
3.3 HMM 声学建模
3.3.1 模型简介
HMM 模型是语音信号时变特征的有参表示法。它由相互关联的两个随机过程
共同描述信号的统计特性，其中一个是隐蔽的(不可观测的)具有有限状态的马尔
可夫链，另一个是与马尔可夫链的每一状态相关联的观察矢量的随机过程(可观
测的)。隐马尔可夫链的特征要靠可观测到的信号特征揭示。这样，语音等时变
信号某一段的特征就由对应状态观察符号的随机过程描述，而信号随时间的变化
由隐马尔可夫链的转移概率描述。HMM 模型的设计如下图 3 所示：

图 3 HMM 模型设计原型
虽然隐马尔可夫模型（HMM）是现在最流行的语音识别模型，然而基本型的
HMM 有一个固有的缺陷，就是它采用状态输出独立假设，影响了 HMM 描述语音信
号时间上帧间相关动态特性的能力。为了弥补这一缺陷，已经有许多改进方法被
[12]
提出。文献 中提到诸如：增加删状态数和在时间轴方向利用回归系数法；使用
线性或非线性预测器法；利用多项式回归函数法；利用条件概率 HMM 的方法；利
用模拟人的听觉顺向时频特性的动态倒谱系数法等。这些提案对于改善传统输出
独立 HMM 的缺陷都是有效的方法，但是它们实现起来较为复杂。
3.3.2 HMM 三个基本问题及其解决方案
1) 已知观察序列 O 和模型   ( A, B,  ) , 如何计算由此产生此观察序列概率
P (0 |  ) ?
这个问题实际上是一个模型评估问题, 因为 P (0 |  ) 反映了观察序列与模型
吻合的程度。在语音识别中, 我们可以通过计算、比较 P (0 |  ) ，从多个模型参
数中选择出与观察序列匹配的最好的模型。为了解决这个问题, 前人已经研究了
向前向后算法。
2) 已知观察序列 O 和模型 , 如何确认一个合理的状态序列, 使之能最佳地
产生 O，即如何选择最佳的状态序列 q  {q1 , q2 ,, qr } ？
这个问题关键是怎样最佳的准则来决定状态的转移。一种可能的最佳准则
是:
| qt*  arg BBN max [ P ((1  i ) |)0 |  ]
这里存在一个问题: 有时候会出现不允许的转移, 即 ai j  0 ，那么对这些 i

和 j 所得到的状态序列就是不可能状态序列也就是说,式子得到的解只是在每个
时刻决定一个最可能的状态,而没考虑整体结构, 相邻的状态和观察序列长度问
[13]
题。针对这个问题,最好的解决方案是 Viterbi 算法, 也是在语音识别过程中
的主要算法。
3) 语音模型训练的好坏直接关系到语音识别系统识别率的高低。为了得到
一个好的模板，往往需要有大量的原始语音数据来训练语音模型。因此，在开始
进行语音识别研究之前， 首先要建立起一个庞大的语音数据库和语料数据库。
一个好的语音数据库包括足够数量、具有不同性别、年龄、口音说话人的声音，
并且必须要有代表性，能均衡地反映实际使用情况。有了语音数据库及语音特征，
就可以建立语音模型，并用语音数据库中的语音来训练这个语音模型。训练过程
是指选择系统的某种最佳状态不断地调整参数 ( A, B,  ) 使得 P (0 |  ) 最大。这是
一个复杂的过程，因为没有解析法可以用来求最大似然模型，所以只能用迭代法
(Baum- Welch) 算法或者使用最佳梯度法。要求计算机有强大的计算能力，并有
很强的理论指导，才能保证得到良好的训练结果。
3.3.3 HMM 的优化
HMM 是到目前为止已有的最强有力的语音识别算法。对语音识别系统而
言,HMM 的输出值通常就是各个帧的声学特征。为了降低模型的复杂度 , 通常
HMM 模型有两个假设前提,一是内部状态的转移只与上一状态有关,一是输出值
只与当前状态或当前状态转移有关。除了这两个假设外，HMM 模型还存在着一些
理论上的假设,其中之一就是,它假设语音是一个严格的马尔科夫过程。我们通常
用从左向右的单向的、带自环的、带跨越的 HMM 拓扑结构来对识别基元建模。例
如,一个音素对应一个三至五状态的 HMM,一个词对应于构成该词的多个音素的
HMM 串,而连续语音则对应于词和静音组合起来的 HMM 串。
非齐次 HMM[14]，将对 HMM 模型方法参数的方法进行了重新优化，假设模型的
状态驻留长度分布函数 {Pi ( )} ，从而导出转移矩阵 {ai j }NXN ，已知 {Pi ( )} ，则：
aij (k )  Pi (  k /   k  1) 

Pi [  k    k  1]
P (  k )
 i
Pi (  k  1)
Pi (  k  1)

3.4 神经网络
目前，有四类主要的神经网语音识别方法[15]。第一类方法是使用具有特殊结
构的神经网络，如：回归网络(Recurrent Net)，时间延迟网络(TDNN)，动态网
络(Dynamic)，这样以便处理时变语音特征序列。第二类方法是典型的 Kohonen
语音识别方法。这种方法基于对特征映射中特征矢量序列的轨迹的处理。它需要
复杂的人工智能规则解释各序列轨迹，且依赖所需识别语言。第三类方法是神经
网络与 HMM(Hidden Markov Model)相结合，以神经网络有效的静态模式分类与
HMM 有效的时变序列建模能力相结合。第四类是以传统的 DTW 算法为基础，然后
用神经网络的并行运算来作为硬件实现的手段。
在文献[16]中，作者谈到人工神经网络能够逼近所有的多元连续函数。通过对
数据样本的学习，神经网络能自动地逼近最佳刻画样本数据规律的函数，而不必
事先设想函数应具备的具体形式，能自动地建立预测模型并作出正确的预测。目
前有许多种人工神经网络的模型，如前馈多层人工神经网络的模型，Hopfield
人工神经网络、模糊神经网络的模型、玻尔兹曼 CBM)人工神经网络的模型。前
馈多层神经网络方法[17]是最早应用于语言识别的研究，也是最成功的。

图 4 语音识别的前馈多层神经网络模型
采用前馈多层神经网络的模型[18]，其网络的拓朴结构见上图 4。该模型中具
有一个输出层，一个输入层，中间还有一个隐含层。输入层各神经元将输入信号
经权重耦合到隐含层的每个节点，隐含层各节点对来自前一层的信号加权和，经
1
Sigmoid 函数 y ( x) 
转换后再耦合到输出层。输出节点将输入信号经
1  ex
Sigmoid 函数后给出网络的输出信号,然后将网络输出信号与期望输出信号(语
音者)进行比较，计算两者之间的误差(RMSE, root-mean-square error)值：
RMSE 

S

P

1

1

(Y

 

 2

)







, Y  ( E  O )

SP


这里 S 和 P 分别表示神经网络的训练样本数和输出层节点的数目， E 和 O 分

别表示第  个输出层节点上的  个样本的期望输出值(语音者)和网络计算输出
值.若 RMSE 大于预定值,网络进行反向传播,依次求出误差信号和误差梯度,并根
据误差梯度修改权重



Wi j (t )  Oi  W j (t  1)
再进行新的权重重新进行正向处理,直到 RMSE 满足要求为止.这里η和μ分别表
示学习步长和控制常数.输出层误差信号  为

  YI  y ' ( xi  ), xi    Oi Wij 


j

同时隐含层误差信号  为

   ( PV Pl  ) y ' ( xPl  )
P









Wij 和 Vij 是权重系数。通过对 S 个样本的训练，得到一组权重系数 Wij 和 Vij 。

用这一组权重系数对我们要预测的参数进行预测，得到预测结果。
4. 语音识别的一些框架
4.1 软插件
[19]
软插件 作为软件的一种集成机制，具有以下特征:1、模块性好，独立性强;2、
可靠性好;3、内部功能的高效实现;4、连接简单，使用方便;5、有封装功能;6、
清晰、简明的说明。
使用软插件模式的例子有很多，如著名的 Java 开发环境 Eclipse 就是一个

最典 型的使用软插件模式的 软件，各种插件集中在 Eclipse 的一个名称为
plugins 的文件夹中，以关.jar 形式打包，通常还都配有相应配置文件 plugin.
xml 用于提供插件的配置参数信息。Eclipse 在启动时自动扫描此文件夹，并装
入各种组件。又如图像处理软件 Photoshop 用于实现图像特效的滤镜，也是软插
件技术装载的。
通过分析，决定使用软插件模式来开发辅助操作部分，使系统可以承受辅助
操作的功能变化。在实现的过程中模仿了 Eclipse 的插件体系，使用的核心技术
是反射。
程序代码在编译后生成可执行的应用程序，.Net 的应用程序结构分为程序域
一程序集一模块一类型一成员几个层次。程序集包含模块，而模块包含类型，类
型又包含成员。反射提供了封装程序集、模块和类型的对象。可以使用反射动态
地创建类型的实例，将类型绑定到现有对象，或从现有对象中获取类型。然后，
可以调用类型的方法或访问其字段和属性。
4.2 POCKETSPHINX 框架
[12]
POCKETSPHINX 是 CMU(Carnegie Mellon University)面向嵌入式设备的开
发的大词汇量连续语音识别(LVCSR)的搜索引擎，该引擎用按 ANSI 标准用 C 语言
实现，支持 GNU／Linux，*BSD，Mac OS X，Android，uCl inux，Windows 及 Windows
CE 等多种平台。
POCKETSPHINX 主 要 针 对 高 斯 混 合 模 型 (HMM) 计 算 方 面 做 了 优 化 。
POCKETSPHINX 把 GMM 分割成 4 层计算：语音帧层、单个高斯层、GMM 层、混合成
分层(特征向量)，分别针对这 4 层计算进行了相关的优化。

参考文献
1 赵力.语音信号处理.第一版,北京:机械工业出版社,2003
2 http://dev.voicecloud.cn/platform.php?vt=1
3 http://news.newhua.com/news/2011/1206/138541.shtml
4 http://www.google.org
5 http://www.cnnic.cn/research/bgxz/ydhwbg/201108/t2011082922658.html
6 马志欣,王宏,李鑫. 语音识别技术综述[J].昌吉学院学报,2006(3):93-97.
7 吕云芳. 基于模板匹配法的语音识别系统研究与基本实现[D]. 河北工业大学,
2005.
8 聂晓飞, 赵禹, 詹庆才. 一种基于模板匹配的语音识别算法[J]. 电子设计工
程, 2011, 19(19):58-60.
9 范崇山, 陈新伟, 罗智荣,等. 典型简单模板匹配语音识别方式技术研究[J].
科技视界, 2017(7):238-239.
10 潘智刚, 姚敏锋, 张晶. 多模板优化的语音识别算法[J]. 电脑知识与技术,
2015(1):146-149.
11 李琼. 基于信号与内容特征的音频判决系统的 研究与实现[D]. 电子科技大
学, 2015.
12 黄瑞. 面向移动终端的语音助手 AudioPhone 的设计与开发[D]. 浙江大学,
2012.
13 袁俊. HMM 连续语音识别中 Viterbi 算法的优化及应用[J]. 电子技术, 2001,
28(2):46-49.
14 王作英. 非齐次语音识别 HMM 模型和 THED 语音识别与理解系统[C]// 全国人
机语音通讯学术会议. 1992:31-954.
15 张立朋, 李立梅. 一种用于语音识别的神经网络[J]. 北京邮电大学学报,
1995(1):31-37.
16 游小微. 语音识别的神经网络方法研究[J]. 浙江师范大学学报(自然科学
版), 2002, 25(3):255-257.
17 Waibel A. Phoneme recognition: Neural network vs. Hidden Markov
models[J]. proceeding from ICASSP, 1988(13):78- 85.
18 Zhang L X, Xia A G, Zhao D L.Predicting chains dimesions from an
artificial neural network model[ J].J Polym Sci; Polym Phys.(PartB), 2000,
38: 3 163- 3 167.
19 孙淑娟, 牟德昆. 软插件模式在 Windows 语音助手中的应用[J]. 潍坊学院
学报, 2011, 11(4):27-31.

